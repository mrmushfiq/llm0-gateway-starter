# LLM0 Gateway Starter

**Open-source, production-ready LLM gateway** with multi-provider support, automatic failover, and intelligent caching.

> **Self-hosted alternative to LiteLLM and Portkey.** Built in Go for performance and reliability. For advanced features like semantic caching and cost-based rate limiting, see [LLM0.ai](https://llm0.ai) *(Coming Soon)*.

---

## âœ¨ Features

### What's Included âœ…

- **ğŸŒ Multi-Provider Support**
  - OpenAI (GPT-4, GPT-4o, GPT-4o-mini)
  - Anthropic (Claude 4, Claude 3.5)
  - Google Gemini (Gemini 2.0, 2.5)
  - Unified OpenAI-compatible API

- **ğŸ”„ Automatic Failover**
  - Preset failover chains (e.g., OpenAI â†’ Anthropic â†’ Gemini)
  - Triggers: 429 rate limits, 5xx errors, timeouts
  - Zero configuration needed

- **ğŸ“¡ Streaming Support (SSE)**
  - Server-Sent Events for real-time responses
  - Works with all three providers
  - Unified format (OpenAI-compatible)

- **ğŸ’¾ Basic Caching**
  - Redis-backed exact-match caching
  - Configurable TTL
  - Cache hit headers

- **ğŸš¦ Token Bucket Rate Limiting**
  - Per-API-key limits
  - Redis Lua scripts (atomic operations)
  - Standard X-RateLimit-* headers

- **ğŸ’° Cost Tracking**
  - Per-request cost calculation
  - Token counting
  - Database-driven model pricing

- **ğŸ“Š Request Logging**
  - PostgreSQL-backed logs
  - Cost, latency, tokens tracked
  - Queryable for analytics

### Coming Soon ğŸš§

- **ğŸ¤– Self-Hosted Models (vLLM + K8s)**
  - **Llama 3.3 8B / Llama 3.1 8B** â€” Sweet spot for K8s self-hosting
    - Fits on a single A100 40GB or T4 16GB (quantized)
    - Strong general performance for SaaS use cases
    - Your "cheap alternative to GPT-4o mini"
  - **Mistral Nemo 12B / Mistral 7B** â€” Apache 2.0, punches above its weight
    - Excellent for coding tasks
    - No usage restrictions
  - **Qwen 2.5 Coder 7B/14B** â€” Best coding-specialized model at small sizes
    - Perfect for SaaS devs building AI features
    - Optimized for developer tooling

- **ğŸ”” Budget Alerts & Notifications** *(Coming Soon)*
  - Multi-threshold alerts (70%, 85%, 100%)
  - Spend forecasting & anomaly detection
  - Multi-channel notifications (email, webhook, Slack, PagerDuty)

---

## ğŸš€ Quick Start

### Prerequisites

- Go 1.21+
- PostgreSQL 15+ (or [Neon](https://neon.tech) free tier)
- Redis 7+
- API keys for OpenAI, Anthropic, and/or Gemini

### Setup Options

**Option A: Automated Setup (Recommended)**
```bash
git clone https://github.com/yourusername/llm0-gateway-starter
cd llm0-gateway-starter

# Run automated setup script (handles everything)
./scripts/setup.sh

# Edit .env with your API keys
# OPENAI_API_KEY=sk-...

# Start the gateway
go run cmd/gateway/main.go
```

**Option B: Manual Setup**
```bash
# 1. Clone and install dependencies
git clone https://github.com/yourusername/llm0-gateway-starter
cd llm0-gateway-starter
go mod download

# 2. Copy environment template
cp .env.example .env
# Edit .env with your credentials

# 3. Start infrastructure
docker-compose up -d
# Docker automatically runs migrations on first start via /docker-entrypoint-initdb.d

# 4. Start the gateway
go run cmd/gateway/main.go
```

**Option C: Without Docker (Using Neon or existing PostgreSQL)**
```bash
# 1. Sign up at https://neon.tech (free tier)
# 2. Create a database
# 3. Copy connection string to .env

# 4. Run migrations manually
psql "postgresql://user:pass@host/db" -f migrations/001_initial_schema.sql

# 5. Start Redis locally or use Upstash (free tier)
# 6. Start the gateway
go run cmd/gateway/main.go
```

**Note:** The test API key `gw_test_abc123` is created by the migration.

Gateway runs at `http://localhost:8080`

---

## ğŸ“– Usage

### Basic Request

```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Authorization: Bearer your-api-key" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o-mini",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```

### Streaming Request

```bash
curl -N -X POST http://localhost:8080/v1/chat/completions \
  -H "Authorization: Bearer your-api-key" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o-mini",
    "messages": [{"role": "user", "content": "Count to 5"}],
    "stream": true
  }'
```

### Response Headers

```http
HTTP/1.1 200 OK
X-Cache-Hit: miss
X-Cost-USD: 0.000009
X-Provider: openai
X-RateLimit-Limit: 100
X-RateLimit-Remaining: 87
X-Latency-Ms: 234
```

**Note:** Use the test API key `gw_test_abc123` for testing (created by migration).

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ HTTP POST
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           LLM Gateway (Go)                      â”‚
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  1. Auth & Rate Limiting                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  2. Cache Check (Redis)                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  3. Provider Detection                   â”‚  â”‚
â”‚  â”‚     (OpenAI / Anthropic / Gemini)        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  4. Failover Logic                       â”‚  â”‚
â”‚  â”‚     (Retry with next provider if fails)  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  5. Streaming or Standard Response       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  6. Cost Tracking & Logging              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼ (One of three)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OpenAI  â”‚  Anthropic   â”‚    Gemini    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ—ï¸ Technical Stack

- **Language:** Go 1.21+ (performance, concurrency, single binary deployment)
- **Database:** PostgreSQL 15+ (request logs, cost tracking, analytics)
- **Cache:** Redis 7+ (exact-match caching, rate limiting)
- **Streaming:** Server-Sent Events (SSE) - OpenAI-compatible format
- **Deployment:** Docker, single binary, cloud-agnostic

---

## ğŸ—„ï¸ Database Migrations

### How Migrations Work

**With Docker (Automatic):**
```bash
docker-compose up -d
# Migrations run automatically on first initialization via /docker-entrypoint-initdb.d
```

**Manual (When needed):**
```bash
# Run migrations directly
psql "$DATABASE_URL" -f migrations/001_initial_schema.sql

# Or inside Docker container
docker exec -i gateway_postgres psql -U gateway -d gateway < migrations/001_initial_schema.sql
```

**Using setup.sh (Recommended):**
```bash
./scripts/setup.sh
# Handles Docker startup + migration in one command
```

### Making Schema Changes

If you need to modify the database schema, we recommend [**Atlas**](https://atlasgo.io) for managing migrations:

```bash
# Install Atlas
brew install ariga/tap/atlas

# Generate migration from schema changes
atlas migrate diff \
  --to "file://migrations/001_initial_schema.sql" \
  --dev-url "docker://postgres/15"

# Apply migrations
atlas schema apply \
  --url "postgres://user:pass@host/db" \
  --to "file://migrations/001_initial_schema.sql"
```

**Why Atlas?**
- âœ… Declarative schema management
- âœ… Automatic migration generation
- âœ… Schema validation and inspection
- âœ… Free for open source projects
- âœ… Works with any PostgreSQL provider

**Alternative:** [golang-migrate](https://github.com/golang-migrate/migrate) is also excellent for version-based migrations.

---

## ğŸ†š Feature Comparison

### Basic vs. Advanced Features

### ğŸ§  **Semantic Caching**

**What this project includes:**
- âœ… Exact-match caching (Redis)
- âœ… ~12-15% cache hit rate
- âœ… Sub-millisecond cache responses

**What [LLM0.ai](https://llm0.ai) *(Coming Soon)* adds:**
- âœ… Semantic similarity matching
- âœ… 36-40% cache hit rate (3x better)
- âœ… 60-89% cost reduction
- âœ… Vector-based similarity search

**Why it matters:**
- "What is AI?" matches "Explain artificial intelligence"
- Users rephrase questions constantly
- Exact-match alone misses 75% of cacheable queries

[Learn more â†’](https://llm0.ai/features/semantic-caching)

---

### ğŸ’° **Cost-Based Rate Limiting**

**What this project includes:**
- âœ… Token bucket rate limiting (requests/minute)
- âœ… Per-API-key limits
- âœ… Redis atomic operations

**What [LLM0.ai](https://llm0.ai) *(Coming Soon)* adds:**
- âœ… Cost-based limits ($X/day per customer)
- âœ… Multi-dimensional tracking (cost + requests + model + labels)
- âœ… Soft degradation (downgrade model instead of blocking)
- âœ… Real-time spend tracking

**Why it matters:**
- 1000 GPT-4 requests â‰  1000 GPT-4o-mini requests (cost difference: 50x)
- Standard rate limiting doesn't protect profit margins
- Power users can destroy your economics

**This project includes:**
- âœ… Token bucket rate limiting (requests/minute)
- âœ… Per-API-key limits
- âœ… Redis atomic operations

**[LLM0.ai](https://llm0.ai) *(Coming Soon)* adds:**
- âœ… **Cost-based limits** ($X/day per customer)
- âœ… **Per-customer tracking** (who costs you what)
- âœ… **Label attribution** (track by feature, team, client)
- âœ… **Real-time dashboards** (spend by customer, model, label)

[Learn more about LLM0's cost-based rate limiting â†’](https://llm0.ai/features/cost-based-limits)

---

### ğŸ“Š **Customer Attribution & Analytics**

**What this project includes:**
- âœ… Request logging (PostgreSQL)
- âœ… Cost and latency tracking
- âœ… Per-model analytics

**What [LLM0.ai](https://llm0.ai) *(Coming Soon)* adds:**
- âœ… Per-customer spend tracking (`X-Customer-ID`)
- âœ… Label-based attribution (`X-LLM0-Feature`, `X-LLM0-Client`)
- âœ… Real-time dashboards ("who costs me money")
- âœ… Budget alerts (70%, 85%, 100% thresholds) *(Coming Soon)*
- âœ… Spend forecasting (predict monthly costs) *(Coming Soon)*
- âœ… Multi-channel notifications (email, webhook, Slack, PagerDuty) *(Coming Soon)*

**Why it matters:**
- SaaS: Track costs per end-user
- Agencies: Track costs per client
- Multi-tenant: Prevent one user from blowing your budget

**This project includes:**
- âœ… Request logging (PostgreSQL)
- âœ… Cost, latency, token tracking
- âœ… Queryable analytics

**[LLM0.ai](https://llm0.ai) adds:**
- âœ… Per-customer spend tracking
- âœ… Multi-dimensional attribution (feature, team, client)
- âœ… Real-time headers (`X-Customer-Spend-Today`)
- âœ… Spend forecasting & anomaly detection
- âœ… Multi-channel alerts (email, webhook, Slack, PagerDuty)

[Learn more about LLM0's analytics â†’](https://llm0.ai/features/analytics)

---

## ğŸ¯ Comparison: Starter vs. LLM0

| Feature | This Starter | [LLM0.ai](https://llm0.ai) |
|---------|--------------|---------------------------|
| **Multi-provider** | âœ… 3 providers | âœ… 3 providers |
| **Failover** | âœ… Basic | âœ… Advanced + configurable |
| **Streaming** | âœ… SSE | âœ… SSE |
| **Caching** | âœ… Exact-match (Redis) | âœ… Exact + **Semantic** |
| **Rate Limiting** | âœ… Token bucket | âœ… Token bucket + **Cost-based** |
| **Cost Tracking** | âœ… Per-request | âœ… Per-request + **Per-customer** |
| **Customer Attribution** | âŒ | âœ… **Multi-dimensional** |
| **Soft Limits** | âŒ | âœ… **Model downgrade** |
| **Label Tracking** | âŒ | âœ… **Feature/team/client** |
| **Budget Alerts** | âŒ | âœ… **Email/webhook/Slack/PagerDuty** |
| **Spend Forecasting** | âŒ | âœ… **Predictive alerts** |
| **Shadow Mode** | âŒ | âœ… **A/B testing** |
| **Scheduled Jobs** | âŒ | âœ… **Automatic maintenance** |
| **Dashboard** | âŒ | âœ… **Real-time analytics** |
| **Support** | Community | âœ… **Priority support** |

---

## ğŸ¤ Contributing

Contributions welcome! See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

**We'd love help with:**
- New providers (Cohere, Mistral, Together AI)
- Failover chain improvements
- Performance optimizations
- Additional test coverage
- Documentation enhancements

---

## ğŸ“œ License

MIT License - Free to use, modify, and distribute.

**Attribution appreciated but not required.**

---

## ğŸ™ Acknowledgments

Created by Mushfiq Rahman [@mrmushfiq](https://github.com/mrmushfiq).

---

## ğŸ”— Links

- **Production Version:** [LLM0.ai](https://llm0.ai)
- [Twitter](https://twitter.com/mushfiq_dev)

---

**â­ If this helped you, please star the repo!**

**Questions? Open an issue or check out the [full version at LLM0.ai](https://llm0.ai)** (Coming soon)
